# nonlinearoptimizationandlearningproject
How do different activation functions, when they are the only one being used in a network, with sole exception of the last layer, which will be a SoftMax, influence in how many epochs a given total loss is achieved?
